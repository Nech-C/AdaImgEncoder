{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoding with Custom Transformer Decoder - Training Prototype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection, CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = load_dataset(\"nlphuji/flickr30k\", split='test').select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModelWithProjection(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CLIP models and processors\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "vision_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Move models to GPU\n",
    "text_model.to('cuda')\n",
    "vision_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(examples):\n",
    "    images = examples[\"image\"]\n",
    "    processed_images = image_processor(images, return_tensors=\"pt\")\n",
    "    processed_images = {k: v.to('cuda') for k, v in processed_images.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_outputs = vision_model(**processed_images)\n",
    "    \n",
    "    return {\"image_embed\": image_outputs.last_hidden_state.cpu().numpy()}\n",
    "\n",
    "def process_text(examples):\n",
    "    # Flatten the list of lists of captions\n",
    "    all_captions = [caption for image_captions in examples[\"caption\"] for caption in image_captions]\n",
    "    \n",
    "    # Tokenize all captions at once\n",
    "    tokenized_text = tokenizer(all_captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_text = {k: v.to('cuda') for k, v in tokenized_text.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_outputs = text_model(**tokenized_text)\n",
    "    \n",
    "    # Get the text embeddings\n",
    "    text_embeds = text_outputs.text_embeds.cpu().numpy()\n",
    "    \n",
    "    # Reshape the embeddings to match the original structure (5 captions per image)\n",
    "    num_images = len(examples[\"caption\"])\n",
    "    reshaped_embeds = text_embeds.reshape(num_images, 5, -1)\n",
    "    \n",
    "    return {\"text_embed\": reshaped_embeds}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e069e4f5f54244479a6634c5ca06f0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process text\n",
    "ds = ds.map(process_text, batched=True, batch_size=32, remove_columns=[\"caption\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96742bc522ee49678bd71cd03168cc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process images\n",
    "ds = ds.map(process_images, batched=True, batch_size=32, remove_columns=[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format for PyTorch\n",
    "ds.set_format(type=\"torch\")\n",
    "\n",
    "# Split the dataset\n",
    "dataset = ds.train_test_split(test_size=0.2, seed=42)\n",
    "train_val_dataset = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    'train': train_val_dataset['train'],\n",
    "    'validation': train_val_dataset['test'],\n",
    "    'test': dataset['test']\n",
    "})\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(final_dataset['train'], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(final_dataset['validation'], batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(final_dataset['test'], batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=768, bias=True)\n",
       "  (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 128\n",
    "vision_dim = 1024\n",
    "clip_dim = 768\n",
    "max_length = 6  # Maximum number of tokens to generate\n",
    "\n",
    "# Define the custom transformer decoder\n",
    "model = CustomImageEncoder(d_model, vision_dim, 8, 512, 0.05, 6, None, 8)\n",
    "\n",
    "# Define the projector\n",
    "projector = nn.Sequential(\n",
    "    nn.Linear(d_model, clip_dim),\n",
    "    nn.LayerNorm(clip_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(clip_dim, clip_dim)\n",
    ")\n",
    "\n",
    "# Move models to GPU\n",
    "model.to('cuda')\n",
    "projector.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomImageEncoder(\n",
      "  (decoder): CustomTransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x CustomTransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.05, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.05, inplace=False)\n",
      "        (dropout2): Dropout(p=0.05, inplace=False)\n",
      "        (dropout3): Dropout(p=0.05, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "# Define hyperparameters\n",
    "num_epochs = 10\n",
    "warmup_steps = 1000  # Adjust as needed\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = optim.AdamW(list(model.parameters()) + list(projector.parameters()), lr=5e-4)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CustomImageEncoder'>\n"
     ]
    }
   ],
   "source": [
    "print(CustomImageEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.transformer import TransformerDecoderLayer\n",
    "\n",
    "class CustomTransformerDecoderLayer(TransformerDecoderLayer):\n",
    "    def __init__(self, d_model, encoded_img_dim, nhead, dim_feedforward=2048,\n",
    "                 dropout=0.1, norm_first=False, batch_first=True):\n",
    "        super(CustomTransformerDecoderLayer, self).__init__(d_model, nhead, dim_feedforward,\n",
    "                                                            dropout, batch_first=batch_first,\n",
    "                                                            norm_first=norm_first)\n",
    "        # Override the multihead_attn layer to attend to encoded image with different dim\n",
    "        self.multihead_attn  = nn.MultiheadAttention(d_model, nhead, dropout, kdim=encoded_img_dim,\n",
    "                                                     vdim=encoded_img_dim, batch_first=batch_first)\n",
    "\n",
    "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor = None,\n",
    "                memory_mask: torch.Tensor = None, tgt_key_padding_mask: torch.Tensor = None, \n",
    "                memory_key_padding_mask: torch.Tensor = None, tgt_is_causal: bool = False, \n",
    "                memory_is_causal: bool = False) -> torch.Tensor:\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n",
    "            x = x + self._mha_block(self.norm2(x), memory, memory_mask,\n",
    "                                    memory_key_padding_mask, memory_is_causal)\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n",
    "            x = self.norm2(x + self._mha_block(x, memory, memory_mask,\n",
    "                                               memory_key_padding_mask, memory_is_causal))\n",
    "            x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _mha_block(self, x: torch.Tensor, mem: torch.Tensor,\n",
    "                   attn_mask: torch.Tensor, key_padding_mask: torch.Tensor,\n",
    "                   is_causal: bool = False) -> torch.Tensor:\n",
    "        x = self.multihead_attn(x, mem, mem,\n",
    "                                attn_mask=attn_mask,\n",
    "                                key_padding_mask=key_padding_mask,\n",
    "                                is_causal=is_causal,\n",
    "                                need_weights=False)[0]\n",
    "        return self.dropout2(x)\n",
    "\n",
    "class CustomTransformerDecoder(nn.TransformerDecoder):\n",
    "    def __init__(self, d_model, d_encoding, nhead, dim_feedforward, dropout, num_layers, norm=None):\n",
    "        decoder_layer = CustomTransformerDecoderLayer(d_model, d_encoding, nhead, dim_feedforward, dropout)\n",
    "        super(CustomTransformerDecoder, self).__init__(decoder_layer, num_layers, norm)\n",
    "\n",
    "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor = None,\n",
    "                memory_mask: torch.Tensor = None, tgt_key_padding_mask: torch.Tensor = None,\n",
    "                memory_key_padding_mask: torch.Tensor = None, tgt_is_causal: bool = False,\n",
    "                memory_is_causal: bool = False) -> torch.Tensor:\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask,\n",
    "                         tgt_is_causal=tgt_is_causal, memory_is_causal=memory_is_causal)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageEncoder(nn.Module):\n",
    "    def __init__(self, d_model, d_encoding, nhead, dim_feedforward, dropout,\n",
    "                 num_layers, output_dim, max_length, norm=None):\n",
    "        super().__init__()\n",
    "        self.decoder = CustomTransformerDecoder(d_model, d_encoding, nhead, dim_feedforward,\n",
    "                                                dropout, num_layers, norm)\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None):\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "    def generate(self, memory, max_length):\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = memory.size(0)\n",
    "\n",
    "        # Initialize the input sequence with start tokens\n",
    "        input_seq = torch.zeros(batch_size, 1, self.d_model, device=device)\n",
    "\n",
    "        for _ in range(max_length - 1):  # -1 because we already have one token\n",
    "            # Create causal mask\n",
    "            tgt_mask = self.generate_square_subsequent_mask(input_seq.size(1)).to(device)\n",
    "\n",
    "            # Generate the next token\n",
    "            output = self.forward(input_seq, memory, tgt_mask=tgt_mask)\n",
    "            next_token = output[:, -1, :].unsqueeze(1)\n",
    "\n",
    "            # Append the next token to the input sequence\n",
    "            input_seq = torch.cat([input_seq, next_token], dim=1)\n",
    "\n",
    "        return input_seq\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wy1677xn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74068ca004fa4e82bc5d03113776049c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment-1</strong> at: <a href='https://wandb.ai/nechs-team/image-encoding-project/runs/wy1677xn' target=\"_blank\">https://wandb.ai/nechs-team/image-encoding-project/runs/wy1677xn</a><br/> View project at: <a href='https://wandb.ai/nechs-team/image-encoding-project' target=\"_blank\">https://wandb.ai/nechs-team/image-encoding-project</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240805_201938-wy1677xn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wy1677xn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nech\\projects\\python_projects\\AdaImgEncoder\\AdaImgEncoder\\wandb\\run-20240805_202211-dzuunhw2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nechs-team/image-encoding-project/runs/dzuunhw2' target=\"_blank\">experiment-1</a></strong> to <a href='https://wandb.ai/nechs-team/image-encoding-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nechs-team/image-encoding-project' target=\"_blank\">https://wandb.ai/nechs-team/image-encoding-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nechs-team/image-encoding-project/runs/dzuunhw2' target=\"_blank\">https://wandb.ai/nechs-team/image-encoding-project/runs/dzuunhw2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training:   0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\Nech\\anaconda3\\envs\\AdaImgEncoder\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Epoch 1/10 - Training: 100%|██████████| 100/100 [06:31<00:00,  3.91s/it]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 25/25 [00:55<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: -0.6688, Val Loss: -1.5888\n",
      "New best model saved with validation loss: -1.5888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 100/100 [06:39<00:00,  4.00s/it]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 25/25 [00:53<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: -1.6566, Val Loss: -1.7085\n",
      "New best model saved with validation loss: -1.7085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 100/100 [06:00<00:00,  3.60s/it]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 25/25 [00:48<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: -1.6966, Val Loss: -1.7137\n",
      "New best model saved with validation loss: -1.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 100/100 [05:48<00:00,  3.48s/it]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 25/25 [00:45<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: -1.7227, Val Loss: -1.7677\n",
      "New best model saved with validation loss: -1.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 100/100 [05:47<00:00,  3.48s/it]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 25/25 [00:44<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: -1.7950, Val Loss: -1.8530\n",
      "New best model saved with validation loss: -1.8530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 100/100 [05:48<00:00,  3.49s/it]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 25/25 [00:43<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: -1.8927, Val Loss: -1.9432\n",
      "New best model saved with validation loss: -1.9432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 100/100 [05:48<00:00,  3.49s/it]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 25/25 [00:43<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: -1.9667, Val Loss: -2.0012\n",
      "New best model saved with validation loss: -2.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 100/100 [05:48<00:00,  3.49s/it]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 25/25 [00:44<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: -2.0197, Val Loss: -2.0392\n",
      "New best model saved with validation loss: -2.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 100/100 [05:44<00:00,  3.44s/it]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 25/25 [00:43<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: -2.0578, Val Loss: -2.0698\n",
      "New best model saved with validation loss: -2.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 100/100 [05:41<00:00,  3.42s/it]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 25/25 [00:43<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: -2.0890, Val Loss: -2.0900\n",
      "New best model saved with validation loss: -2.0900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b283f6a23b4268b32c41e871f17edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▃▃▃▂▂▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▆▆▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>train_loss</td><td>-2.089</td></tr><tr><td>val_loss</td><td>-2.08999</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment-1</strong> at: <a href='https://wandb.ai/nechs-team/image-encoding-project/runs/dzuunhw2' target=\"_blank\">https://wandb.ai/nechs-team/image-encoding-project/runs/dzuunhw2</a><br/> View project at: <a href='https://wandb.ai/nechs-team/image-encoding-project' target=\"_blank\">https://wandb.ai/nechs-team/image-encoding-project</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240805_202211-dzuunhw2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"image-encoding-project\", name=\"experiment-1\")\n",
    "\n",
    "# Log hyperparameters\n",
    "wandb.config.update({\n",
    "    \"epochs\": num_epochs,\n",
    "    \"batch_size\": train_loader.batch_size,\n",
    "    \"initial_learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"model\": model.__class__.__name__,\n",
    "    \"max_length\": max_length,\n",
    "    \"d_model\": d_model\n",
    "})\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    projector.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        image_embed = batch[\"image_embed\"].to('cuda')\n",
    "        text_embeds = batch[\"text_embed\"].to('cuda')  # Assume this contains 5 text embeddings per image\n",
    "        \n",
    "        # Forward pass\n",
    "        generated_sequence = model.generate(image_embed, max_length)\n",
    "        \n",
    "        # Project each token and compute loss\n",
    "        loss = 0\n",
    "        for i in range(1, generated_sequence.size(1)):\n",
    "            projected_token = projector(generated_sequence[:, i, :])\n",
    "            \n",
    "            # Compute similarity between projected token and each of the 5 captions\n",
    "            for j in range(5):\n",
    "                similarity = F.cosine_similarity(projected_token, text_embeds[:, j, :], dim=1)\n",
    "                # We want to maximize similarity, so we minimize negative similarity\n",
    "                loss -= similarity.mean() * i / (max_length - 1)\n",
    "        \n",
    "        loss /= 5  # Average loss over the 5 captions\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    projector.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            image_embed = batch[\"image_embed\"].to('cuda')\n",
    "            text_embeds = batch[\"text_embed\"].to('cuda')\n",
    "            \n",
    "            # Forward pass\n",
    "            generated_sequence = model.generate(image_embed, max_length)\n",
    "            \n",
    "            # Project each token and compute loss\n",
    "            loss = 0\n",
    "            for i in range(1, generated_sequence.size(1)):\n",
    "                projected_token = projector(generated_sequence[:, i, :])\n",
    "                \n",
    "                # Compute similarity between projected token and each of the 5 captions\n",
    "                for j in range(5):\n",
    "                    similarity = F.cosine_similarity(projected_token, text_embeds[:, j, :], dim=1)\n",
    "                    loss -= similarity.mean() * i / (max_length - 1)\n",
    "            \n",
    "            loss /= 5  # Average loss over the 5 captions\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    \n",
    "    # Save the best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'projector_state_dict': projector.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'projector_state_dict': projector.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'custom_transformer_decoder_model.pth')\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_processor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.5873,  0.5873,  0.6165,  ...,  0.0617,  0.0471, -0.0259],\n",
       "          [ 0.5727,  0.5727,  0.6603,  ...,  0.1201,  0.0763,  0.0909],\n",
       "          [ 0.5873,  0.5435,  0.6165,  ...,  0.0325,  0.1201,  0.0617],\n",
       "          ...,\n",
       "          [ 1.8719,  1.8573,  1.8719,  ...,  1.3902,  1.4340,  1.4194],\n",
       "          [ 1.8281,  1.8719,  1.8427,  ...,  1.4486,  1.4340,  1.5070],\n",
       "          [ 1.8573,  1.9011,  1.8281,  ...,  1.3756,  1.3610,  1.4486]],\n",
       "\n",
       "         [[-1.3169, -1.3019, -1.3169,  ..., -1.4970, -1.4369, -1.4820],\n",
       "          [-1.2418, -1.2718, -1.2268,  ..., -1.4369, -1.4669, -1.4519],\n",
       "          [-1.2568, -1.3169, -1.2268,  ..., -1.4669, -1.4069, -1.4519],\n",
       "          ...,\n",
       "          [ 0.1239,  0.1089,  0.1239,  ..., -0.7016, -0.6865, -0.6865],\n",
       "          [ 0.0789,  0.0939,  0.0488,  ..., -0.6565, -0.6865, -0.6115],\n",
       "          [ 0.0939,  0.1089,  0.0038,  ..., -0.7766, -0.7316, -0.6115]],\n",
       "\n",
       "         [[-0.4848, -0.4137, -0.3853,  ..., -0.9541, -0.8545, -0.8545],\n",
       "          [-0.4137, -0.4706, -0.3711,  ..., -0.8119, -0.8545, -0.7834],\n",
       "          [-0.3284, -0.4422, -0.3853,  ..., -0.8688, -0.8119, -0.8830],\n",
       "          ...,\n",
       "          [ 1.5771,  1.6482,  1.6340,  ...,  0.9088,  0.9514,  0.8945],\n",
       "          [ 1.6198,  1.6055,  1.6055,  ...,  0.8661,  0.8092,  0.7950],\n",
       "          [ 1.6624,  1.6766,  1.5487,  ...,  0.7950,  0.8661,  0.8519]]]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image['pixel_values'].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = vision_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input.last_hidden_state, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 128])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_output = projector(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\"two cats on a red background\", \"cat\", \"red\", \"remotes\"]\n",
    "text_input = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "text_input = {k: v.to('cuda') for k, v in text_input.items()}\n",
    "text_output = text_model(**text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_output.text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity for each token with each caption\n",
    "# text_out: torch.Size([4, 768])\n",
    "# image_out: torch.Size([1, 8, 768])\n",
    "\n",
    "similarity = F.cosine_similarity(projected_output, text_output.text_embeds.unsqueeze(1), dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3084, 0.5018, 0.5014, 0.5012, 0.5012, 0.5011, 0.5011, 0.5011],\n",
      "        [0.3872, 0.6365, 0.6353, 0.6351, 0.6350, 0.6350, 0.6349, 0.6349],\n",
      "        [0.3236, 0.5169, 0.5157, 0.5154, 0.5153, 0.5152, 0.5152, 0.5151],\n",
      "        [0.2977, 0.5974, 0.5964, 0.5963, 0.5962, 0.5962, 0.5961, 0.5961]],\n",
      "       device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m----> 5\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m         image_embed \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to('cuda')\n",
    "        attention_mask = batch[\"attention_mask\"].to('cuda')\n",
    "        image_embed = batch[\"image_embed\"].to('cuda')\n",
    "        \n",
    "        start_token = torch.zeros(image_embed.shape[0], 1, d_model).to('cuda')\n",
    "        output = model(start_token, image_embed, tgt_mask=None, memory_mask=None)\n",
    "        output = projector(output[:,1:,:])\n",
    "        \n",
    "        target = torch.ones(output.shape[0]).to('cuda')\n",
    "        loss = criterion(output.view(-1, clip_dim), image_embed.view(-1, clip_dim), target)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdaImgEncoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
